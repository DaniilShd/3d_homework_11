{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import open3d.ml.torch as ml3d\n",
    "import open3d.ml as o3dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n",
      "True\n",
      "1.26.4\n",
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(np.__version__)\n",
    "print(o3d.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# poss_dataset.py\n",
    "import numpy as np\n",
    "import yaml\n",
    "import open3d.ml.torch as ml3d\n",
    "\n",
    "# POSS (17)\n",
    "POSS_LABELS = {\n",
    "    0: \"unlabeled\",\n",
    "    4: \"1 person\",\n",
    "    5: \"2+ person\",\n",
    "    6: \"rider\",\n",
    "    7: \"car\",\n",
    "    8: \"trunk\",\n",
    "    9: \"plants\",\n",
    "    10: \"traffic sign 1\", # standing sign\n",
    "    11: \"traffic sign 2\", # hanging sign\n",
    "    12: \"traffic sign 3\", # high/big hanging sign\n",
    "    13: \"pole\",\n",
    "    14: \"trashcan\",\n",
    "    15: \"building\",\n",
    "    16: \"cone/stone\",\n",
    "    17: \"fence\",\n",
    "    21: \"bike\",\n",
    "    22: \"ground\"} # class definition\n",
    "\n",
    "class POSSDataset(ml3d.datasets.SemanticKITTI):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        poss_yaml_path: str, # <--- poss.yaml\n",
    "        name: str = \"poss\",\n",
    "        cache_dir: str = \"./logs/cache_poss\",\n",
    "        use_cache: bool = False,\n",
    "        class_weights=None,\n",
    "        ignored_label_inds=None,\n",
    "        test_result_folder=None,\n",
    "        test_split=None,\n",
    "        training_split=None,\n",
    "        validation_split=None,\n",
    "        all_split=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(dataset_path=dataset_path,\n",
    "                         name=name,\n",
    "                         cache_dir=cache_dir,\n",
    "                         use_cache=use_cache,\n",
    "                         class_weights=class_weights,\n",
    "                         ignored_label_inds=ignored_label_inds or [],\n",
    "                         test_result_folder=test_result_folder,\n",
    "                         training_split=training_split or [\"00\",\"01\",\"02\",\"03\"],\n",
    "                         validation_split=validation_split or [\"04\"],\n",
    "                         test_split=test_split or [\"05\"],\n",
    "                         all_split=all_split or [\"00\",\"01\",\"02\",\"03\",\"04\",\"05\"],\n",
    "                         **kwargs)\n",
    "\n",
    "        self._poss_yaml_path = poss_yaml_path\n",
    "        with open(self._poss_yaml_path, \"r\") as f:\n",
    "            DATA = yaml.safe_load(f)\n",
    "\n",
    "        self.label_to_names = self.get_label_to_names()\n",
    "        self.num_classes = len(self.label_to_names)\n",
    "\n",
    "        # learning_map_inv: train_id -> raw_id\n",
    "        remap_dict_inv = DATA[\"learning_map_inv\"]\n",
    "        max_key = max(remap_dict_inv.keys()) if remap_dict_inv else 0\n",
    "        remap_lut = np.zeros((max_key + 100), dtype=np.int32)\n",
    "        remap_lut[list(remap_dict_inv.keys())] = list(remap_dict_inv.values())\n",
    "\n",
    "        # learning_map: raw_id -> train_id\n",
    "        remap_dict = DATA[\"learning_map\"]\n",
    "        max_key_val = max(remap_dict.keys()) if remap_dict else 0\n",
    "        remap_lut_val = np.zeros((max_key_val + 100), dtype=np.int32)\n",
    "        remap_lut_val[list(remap_dict.keys())] = list(remap_dict.values())\n",
    "\n",
    "        self.remap_lut = remap_lut\n",
    "        self.remap_lut_val = remap_lut_val\n",
    "\n",
    "    @staticmethod\n",
    "    def get_label_to_names():\n",
    "        return dict(POSS_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = POSSDataset(\n",
    "    dataset_path=\"./SemanticPOSS_dataset\",\n",
    "    poss_yaml_path=\"randlanet_poss.yml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points shape: (66658, 3)\n",
      "Labels shape: (66658,)\n"
     ]
    }
   ],
   "source": [
    "split = ds.get_split(\"train\")\n",
    "sample = split.get_data(0)\n",
    "points = sample[\"point\"].astype(np.float32)\n",
    "label = sample[\"label\"].astype(np.int32)\n",
    "\n",
    "print(f\"Points shape: {points.shape}\")\n",
    "print(f\"Labels shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ds.label_to_names\n",
    "num_classes = int(max(names.keys())) + 1\n",
    "rng = np.random.default_rng(0)\n",
    "palette = rng.random((num_classes, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_xyz = sample[\"point\"].astype(np.float32)   # (N, 4) - координаты + интенсивность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_infer = {\n",
    "    \"point\": points_xyz,      # Основной тензор точек для модели\n",
    "    \"full_point\": points_xyz  # ОБЯЗАТЕЛЬНОЕ ПОЛЕ для SemanticSegmentation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность points_xyz: (66658, 3)\n",
      "Количество признаков: 3\n"
     ]
    }
   ],
   "source": [
    "points_xyz = sample[\"point\"].astype(np.float32)\n",
    "print(f\"Размерность points_xyz: {points_xyz.shape}\")  # Должно быть (N, 4)\n",
    "print(f\"Количество признаков: {points_xyz.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность тестовых данных (папка 04): (66534, 3)\n"
     ]
    }
   ],
   "source": [
    "# Проверьте размерность в тестовых данных\n",
    "test_split = ds.get_split(\"test\")\n",
    "sample = test_split.get_data(0)  # первый файл из папки 04\n",
    "points = sample[\"point\"].astype(np.float32)\n",
    "print(f\"Размерность тестовых данных (папка 04): {points.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels: 4\n",
      "num_classes: 15\n",
      "ignored_label_inds: [0]\n",
      "dataset_path: None\n",
      "test_split: None\n",
      "training_split: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"in_channels: {cfg.model.get('in_channels')}\")\n",
    "print(f\"num_classes: {cfg.model.get('num_classes')}\")\n",
    "print(f\"ignored_label_inds: {cfg.model.get('ignored_label_inds')}\")\n",
    "print(f\"dataset_path: {cfg.get('dataset_path')}\")\n",
    "print(f\"test_split: {cfg.get('test_split')}\")\n",
    "print(f\"training_split: {cfg.get('training_split')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Из за того что у меня стоит видеокарта нового поколения 5070 есть проблемы с совместью библиотек. Ниже начал обучение на cpu. По причине долгого обучения на cpu с вашего позволения не буду обучать модель до конца. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 5070 Ti\n",
      "Memory: 16.6 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Загружаем конфигурацию\n",
    "cfg_file = 'randlanet_poss.yml'\n",
    "cfg = o3dml.utils.Config.load_from_file(cfg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Получаем классы по именам из конфига\n",
    "DatasetClass = POSSDataset  # Используем наш кастомный класс\n",
    "ModelClass = o3dml.utils.get_module(\"model\", cfg.model.name, \"torch\")\n",
    "PipelineClass = o3dml.utils.get_module(\"pipeline\", cfg.pipeline.name, \"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем датасет POSS...\n"
     ]
    }
   ],
   "source": [
    "# 3. Создаем экземпляр датасета, используя параметры из конфига\n",
    "print(\"Загружаем датасет POSS...\")\n",
    "# Извлекаем путь к датасету и другие параметры\n",
    "dataset_path = cfg.dataset.pop('dataset_path', None)\n",
    "dataset = DatasetClass(dataset_path, \n",
    "                        \"randlanet_poss.yml\",\n",
    "                        **cfg.dataset)  # POSSDataset сам прочитает YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создаем модель и pipeline...\n"
     ]
    }
   ],
   "source": [
    "# 4. Создаем модель и пайплайн\n",
    "print(\"Создаем модель и pipeline...\")\n",
    "model = ModelClass(**cfg.model)\n",
    "\n",
    "# Убедимся, что параметр 'dataset' (как строка) не передается в пайплайн\n",
    "pipeline_cfg = cfg.pipeline.copy()\n",
    "# Если он там есть, удаляем, т.к. передаем объект датасета явно\n",
    "pipeline_cfg.pop('dataset', None)\n",
    "\n",
    "pipeline = PipelineClass(\n",
    "    model=model,\n",
    "    dataset=dataset,  # Передаем объект датасета\n",
    "    device='cpu',    # или 'cpu'\n",
    "    **pipeline_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем обучение...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 994/994 [23:55<00:00,  1.44s/it]\n",
      "validation: 100%|██████████| 250/250 [02:45<00:00,  1.51it/s]\n",
      "training:  18%|█▊        | 175/994 [04:09<19:28,  1.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 5. Обучение модели\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mНачинаем обучение...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/daniil/d73a71ee-06a1-4243-882c-225fb09917a1/daniil/Learn/Semestr_3/3D/HomeWork/Homework_11/.venv/lib/python3.11/site-packages/open3d/_ml3d/torch/pipelines/semantic_segmentation.py:411\u001b[39m, in \u001b[36mSemanticSegmentation.run_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28mself\u001b[39m.losses = []\n\u001b[32m    409\u001b[39m model.trans_point_sampler = train_sampler.get_point_sampler()\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/daniil/d73a71ee-06a1-4243-882c-225fb09917a1/daniil/Learn/Semestr_3/3D/HomeWork/Homework_11/.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/daniil/d73a71ee-06a1-4243-882c-225fb09917a1/daniil/Learn/Semestr_3/3D/HomeWork/Homework_11/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/daniil/d73a71ee-06a1-4243-882c-225fb09917a1/daniil/Learn/Semestr_3/3D/HomeWork/Homework_11/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/daniil/d73a71ee-06a1-4243-882c-225fb09917a1/daniil/Learn/Semestr_3/3D/HomeWork/Homework_11/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/daniil/d73a71ee-06a1-4243-882c-225fb09917a1/daniil/Learn/Semestr_3/3D/HomeWork/Homework_11/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/daniil/d73a71ee-06a1-4243-882c-225fb09917a1/daniil/Learn/Semestr_3/3D/HomeWork/Homework_11/.venv/lib/python3.11/site-packages/open3d/_ml3d/torch/dataloaders/torch_dataloader.py:85\u001b[39m, in \u001b[36mTorchDataloader.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     82\u001b[39m     data = dataset.get_data(index)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m inputs = {\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m: data, \u001b[33m'\u001b[39m\u001b[33mattr\u001b[39m\u001b[33m'\u001b[39m: attr}\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/daniil/d73a71ee-06a1-4243-882c-225fb09917a1/daniil/Learn/Semestr_3/3D/HomeWork/Homework_11/.venv/lib/python3.11/site-packages/open3d/_ml3d/torch/models/randlanet.py:220\u001b[39m, in \u001b[36mRandLANet.transform\u001b[39m\u001b[34m(self, data, attr, min_possibility_idx)\u001b[39m\n\u001b[32m    216\u001b[39m input_up_samples = []\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.num_layers):\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# TODO: Replace with Open3D KNN\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     neighbour_idx = \u001b[43mDataProcessing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mknn_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     sub_points = pc[:pc.shape[\u001b[32m0\u001b[39m] // cfg.sub_sampling_ratio[i], :]\n\u001b[32m    223\u001b[39m     pool_i = neighbour_idx[:pc.shape[\u001b[32m0\u001b[39m] // cfg.sub_sampling_ratio[i], :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/daniil/d73a71ee-06a1-4243-882c-225fb09917a1/daniil/Learn/Semestr_3/3D/HomeWork/Homework_11/.venv/lib/python3.11/site-packages/open3d/_ml3d/datasets/utils/dataprocessing.py:101\u001b[39m, in \u001b[36mDataProcessing.knn_search\u001b[39m\u001b[34m(support_pts, query_pts, k)\u001b[39m\n\u001b[32m     99\u001b[39m nns = o3c.nns.NearestNeighborSearch(o3c.Tensor.from_numpy(support_pts))\n\u001b[32m    100\u001b[39m nns.knn_index()\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m idx, dist = \u001b[43mnns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mknn_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo3c\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_pts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m idx.numpy().astype(np.int32)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 5. Обучение модели\n",
    "print(\"Начинаем обучение...\")\n",
    "pipeline.run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Валидация\n",
    "print(\"\\nНачинаем валидацию...\")\n",
    "pipeline.run_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Тестирование и инференс (пример)\n",
    "print(\"\\nНачинаем тестирование...\")\n",
    "test_split = dataset.get_split(\"test\")\n",
    "if len(test_split) > 0:\n",
    "    sample = test_split.get_data(0)\n",
    "    data = {\n",
    "        \"point\": sample[\"point\"].astype(np.float32),\n",
    "        \"full_point\": sample[\"point\"].astype(np.float32)\n",
    "    }\n",
    "    print(\"Запуск инференса...\")\n",
    "    result = pipeline.run_inference(data)\n",
    "    print(f\"Размер предсказаний: {result['predict_labels'].shape}\")\n",
    "    print(f\"Уникальные классы в предсказаниях: {np.unique(result['predict_labels'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homework-10-py3.11 (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
